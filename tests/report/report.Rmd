---
title: "Report of experiments"
author: "O. Denas"
date: "12/28/2016"
output:
  pdf_document: 
    number_sections: yes
    toc: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\newpage
# Input properties

For various types of inputs ("mut_XMs_YMt_Z" means `s` and `t` are random identical strings of length X, and Y million respectively with mutations inserted every Z characters. "rnd_XMs_YMt" means `s` and `t` are random strings of length X, and Y million respectively)  run the MS algorithm and count the number of 

 * consecutive `parent()` calls during the `runs` construction.
 * consecutive `wl()` calls during the `ms` construction. 
 * the number of 1s in the `runs` bit vector
 * double rank calls that fail (i.e the search down the WT is interrupted prior to reaching a leaf)
 * the number of maximal repeats


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
source('utils.R')
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
stats_path = "../input_stats_data/stats.csv"
parent_data <- read_ds(stats_path, TRUE) %>% filter(measuring == "consecutive_parent_calls", where=="runs")

p <- (ggplot(parent_data) + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5), strip.text.x = element_text(size = 7)) + 
        labs(subtitle = "Counting consecutive parent() calls"))

p + geom_bar(mapping = aes(x = factor(as.numeric(key)), y = value), stat = "identity") + facet_wrap(~inp_type, scales = "free")
rm(p, parent_data)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
wl_data <- read_ds(stats_path, TRUE) %>% filter(measuring == "consecutive_wl_calls", where=="ms")

p <- (ggplot(wl_data) + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5), strip.text.x = element_text(size = 7)) + 
        labs(subtitle = "Counting consecutive parent() calls"))

p + geom_bar(mapping = aes(x = factor(as.numeric(key)), y = value), stat = "identity") + facet_wrap(~inp_type, scales = "free")
rm(p, wl_data)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
drf_data <- read_ds(stats_path, TRUE) %>% filter(measuring == "double_rank_niter")

kable(drf_data %>% spread(key = key, value = value) %>% select(b_path, fail, nofail) %>% mutate(perc = round(100 * fail / nofail, 2)) %>% arrange(perc), caption="Double rank iterations that fail for various input types.")

#kable(drf_data %>% group_by(b_path) %>% summarise(total_calls = sum(as.numeric(key)), fail_calls = sum(value)) %>% mutate(perc = round(100 * fail_calls / total_calls, 2)), caption="Double rank calls that fail for various input types.")
rm(drf_data)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
vcomp_data <- read_ds(stats_path, TRUE) %>% filter(measuring == "vector_composition", where=="runs") %>% mutate(vector_value = ifelse(key == 1, "one", "zero"))

kable(vcomp_data %>% select(vector_value, value, inp_type) %>% spread(key = vector_value, value = value) %>% mutate(zero_perc = round(100 * zero / one, 2)), caption="Composition of the `runs` vector for various input types.")
rm(vcomp_data)
```


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=5}
maxrep_data <- read_ds(stats_path, TRUE) %>% filter(measuring == "vector_composition", where=="maxrep")

kable(maxrep_data %>% select(key, value, inp_type) %>% spread(key = key, value = value) %>% mutate(non_maximal_perc = round(100 * non_maximal / maximal, 2)), caption="Composition of the `B` vector (containing ends of maximal repeats) for various input types.")
rm(stats_path, maxrep_data)
```

\newpage
# Current performance
```{r, echo=FALSE, warning=FALSE, message=FALSE}
ds <- read_ds("../current_performance/res.csv")
parse_lazy <- function(x)  sapply(x, function(s) substr(s, 2, 2))
parse_fail <- function(x)  sapply(x, function(s) substr(s, 5, 5))
parse_maxrep <- function(x)  sapply(x, function(s) substr(s, 8, 8))

kable(merge(filter(ds, item == "ms_maxrep") %>% select(label, value),
            filter(ds, item == "total_time") %>% select(label, value), 
            by = c("label"), all.y = TRUE) %>% 
        mutate(maxrep_s = value.x / 1000, total_s = value.y / 1000,
               lazy = parse_lazy(label), 
               fail = parse_fail(label), 
               maxrep = parse_maxrep(label),
               tot_minus_maxrep_s = total_s - ifelse(is.na(maxrep_s), 0, maxrep_s)) %>%
        select(lazy, fail, maxrep, total_s, maxrep_s, tot_minus_maxrep_s) %>% 
        arrange(total_s), caption=sprintf("Run time in seconds, on random input with |s| = %dMB, |t| = %dMB", ds$len_s[1] / 10^6, ds$len_t[1] / 10^6))

rm(ds, parse_lazy, parse_fail, parse_maxrep)
```

\newpage
# Double vs. single rank
## Rank support optimization
The optimization occurs first at `rank_support_v.hpp` where we avoid recomputing a major block for intervals that are going to fall on the same major block anyways. 

The condition that checks whether endpoints $(i, j)$ of an interval end up in the same major block is `bool((i>>8) == (j>>8))`.

### Code
The single rank and double rank implementations in sdsl: rank_support_v.hpp [link](https://github.com/odenas/indexed_ms/blob/master/sdsl-lite/include/sdsl/rank_support_v.hpp)

```
// RANK(idx)
const uint64_t* p = m_basic_block.data() + ((idx>>8)&0xFFFFFFFFFFFFFFFEULL);
return *p + ((*(p+1)>>(63 - 9*((idx&0x1FF)>>6)))&0x1FF) +
      (idx&0x3F ? trait_type::word_rank(m_v->data(), idx) : 0);

// DOUBLE RANK OD(i, j)
if((i>>8) == (j>>8)){
  const uint64_t* p = m_basic_block.data() + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
  res.first  = *p + ((*(p+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
            (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0); 
  res.second = *p + ((*(p+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
            (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0); 
} else {
  const uint64_t* p = m_basic_block.data() + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
  res.first  = *p + ((*(p+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
            (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0); 
  p -= (((i>>8)&0xFFFFFFFFFFFFFFFEULL) - ((j>>8)&0xFFFFFFFFFFFFFFFEULL));
  res.second = *p + ((*(p+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
            (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0); 
}
return res

// DOUBLE RANK FC(i, j)
const uint64_t* b = m_basic_block.data();
const uint64_t* pi = b + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
const uint64_t* pj = b + ((j>>8)&0xFFFFFFFFFFFFFFFEULL);

return (*pi + ((*(pi+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
              (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0),
      *pj + ((*(pj+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
              (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0));
```
### Performance
The FC implementation seems to work better and will be adopted from now on.
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval = FALSE}
od <- read_ds('../single_rank_vs_double_rank/rank_timing.csv.od') %>% filter(item != "dstruct") %>% mutate(label='OD')
fc <- read_ds('../single_rank_vs_double_rank/rank_timing.csv.fc') %>% filter(item != "dstruct") %>% mutate(label='FC')

dd <- rbind(fc, od) %>% group_by(item, label) %>% summarise(avg_time = mean(time_ms), sd_time = sd(time_ms))
kable(dd, digits=2, caption="Time (in ms) of 500K calls to `wl()` based on `single_rank()` or `double_rank()` methods on 100MB random DNA input; Mean/sd over 20 repetitions.")
```

## Weiner link tricks
### Sandbox performance
TODO: describe dataset and tests
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval = FALSE}
ds <- (read_ds('../single_rank_vs_double_rank/sandbox/rank_timing.csv') %>% 
         mutate(wl_presence = ifelse(has_wl, "has_wl", "no_wl"), 
                interval_width = ifelse(close, "same_block", "different_block")) %>%
         select(method, wl_presence, interval_width, time_ms, char, ntrial) %>% filter(char != 't'))

ds %>% group_by(method, interval_width, wl_presence) %>% summarise(avg_time = mean(time_ms))

ggplot(ds, aes(method, time_ms, color=interval_width)) + 
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(shape=1, alpha=0.5, width=0.1, size = 3) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position=c(.9, .8)) +
  facet_wrap(~wl_presence) 
  labs(title = "Sandbox test") + ylab('milliseconds')
```


## Performance

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=3}
dds <- read_ds('../single_rank_vs_double_rank/rank_timing.csv')
dds$b_path <- factor(dds$b_path, levels = (sorted_bpaths(dds$b_path) %>% arrange(inp_type, mu_nr))$b_path)

p <- ggplot(data = dds)
p + stat_summary(mapping = aes(b_path, time_ms, color=item), 
                 fun.ymin = min,  fun.ymax = max, fun.y = median) + coord_flip()

kable(read_ds('../single_rank_vs_double_rank/rank_timing.csv') %>% filter(item != "dstruct") %>% select(b_path, item, time_ms) %>% group_by(b_path, item) %>% summarize(avg_time = mean(time_ms), sd_time = sd(time_ms)))


```

\newpage

# Maxrep
## Maxrep construction
```{r, echo=FALSE}
optimized <- c(1020, 1023, 999, 1020, 1015)
existing <- c(1098, 1116, 1120, 1094, 1100)
```

Applying the first optimization we get `r round(100 * (mean(existing) - mean(optimized)) / mean(existing), 0)`% improvement on a (ran of a 1MB input string).

```
# EXISTING CODE
denas@denas-osx:$ for i in 1 2 3 4 5; \
do compute_maxrep -answer 0 -load_cst 0 -s_path datasets/synthetic/rnd_1Ms_5Mt.s; \
done  2>&1 | grep mill
 * computing MAXREP DONE ( 1098 milliseconds)
 * computing MAXREP DONE ( 1116 milliseconds)
 * computing MAXREP DONE ( 1120 milliseconds)
 * computing MAXREP DONE ( 1094 milliseconds)
 * computing MAXREP DONE ( 1100 milliseconds)
denas@denas-osx:$ 

# OPTIMIZED CODE
denas@denas-osx:$ for i in 1 2 3 4 5; \
do compute_maxrep -answer 0 -load_cst 0 -s_path datasets/synthetic/rnd_1Ms_5Mt.s; \
done  2>&1 | grep mill
 * computing MAXREP DONE ( 1020 milliseconds)
 * computing MAXREP DONE ( 1023 milliseconds)
 * computing MAXREP DONE ( 999 milliseconds)
 * computing MAXREP DONE ( 1020 milliseconds)
 * computing MAXREP DONE ( 1015 milliseconds)
```

## Performance
```{r, echo=FALSE, warning=FALSE, message=FALSE}
ds <- read_ds('../use_maxrepeat/res.csv') %>% filter(measuring == "time")
ntrials <- 8
```
The figure below shows `r ntrials` runs of the program with and without the use of the `maxrep` (or `B`) vector. The plot shows times (in seconds) for the construction of the `ms` bitvector. The table below that, shows the time (in seconds) to construct the `maxrep` vector. The input data is random and has |s|=`r sprintf("%dMB", ds$len_s[1] / 10^6)` and |t|=`r sprintf("%dMB", ds$len_t[1] / 10^6)`.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=3}

ggplot((ds %>% filter(item == "ms_bvector") %>% select(label, value, item)), aes(label, value / 1000)) + geom_boxplot() + geom_jitter(alpha = 0.5)
kable(ds %>% filter(item == "ms_maxrep", label == "double_rank_maxrep") %>% group_by(label) %>% summarize(avg = round(mean(value / 1000)), se = round(sd(value / 100))),
      caption = "Time to build the maxrep bit vector")
```

\newpage

# Lazy vs non-lazy
## Code
The lazy and non-lazy versions differ in a couple of lines of code as follows

```
if(flags.lazy){
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.lazy_wl(v, c); 
            h_star++;
        }   
    }   
    if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
        st.lazy_wl_followup(v);
} else { // non-lazy weiner links
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.wl(v, c); 
            h_star++;
        }   
    }   
}
```
\newpage
## Performance
```{r, echo=FALSE, warning=FALSE, message=FALSE}
dds <- (read_ds('../lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv', TRUE) %>% filter(measuring == "time", item == 'ms_bvector') %>% select(len_s, len_t, value, item, label, b_path))
dds$b_path <- factor(dds$b_path, levels = (sorted_bpaths(dds$b_path) %>% arrange(inp_type, mu_nr))$b_path)

ggplot(dds, aes(b_path, value/1000, color=label)) + 
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(shape=1, alpha=0.5, width=0.1) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position=c(.9, .8)) +
  #facet_wrap(~item) +
  labs(title = "MS vector construction time") + ylab('seconds') + xlab("input type") 
rm(dds)
```

The right panel shows the time to construct the `runs` vector. This stage is the same for both versions and is shown as a control. On the left panel it can be seen that speedup correlates positively with both the size of the indexed string and the mutation period.

\newpage
## Sandbox timing
Measure the time of 10k repetitions of 

 * (lazy) $n$ consecutive `lazy_wl()` calls followed by a `lazy_wl_followup()`
 * (nonlazy) $n$ consecutive `wl()` calls
 * (lazy_nf) $n$ consecutive `lazy_wl()` calls

```
// lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
    st.lazy_wl_followup(v);
...
// non-lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.wl(v, s_rev[k--]);
...
// lazy_nf
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
sandbox_ds <- read_ds('../lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", s==1e9, nwlcalls<100)

fit_lazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy")))
fit_nonlazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="nonlazy")))
fit_lazy_nf <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy_nf")))

ggplot(sandbox_ds, aes(nwlcalls, time_ms,color=item)) + geom_point() + 
  geom_abline(intercept = coef(fit_lazy)[1], slope = coef(fit_lazy)[2], color='red', alpha=0.5) + 
  geom_abline(intercept = coef(fit_nonlazy)[1], slope = coef(fit_nonlazy)[2], color='blue', alpha=0.5) +
  geom_abline(intercept = coef(fit_lazy_nf)[1], slope = coef(fit_lazy_nf)[2], color='green', alpha=0.5) +
  labs(title = "indexed input size 1G", 
       subtitle=sprintf("lazy: %.2f + %.4f*n; nonlazy: %.2f + %.4f*n; lazy_nf: %.2f + %.4f*n", 
                        coef(fit_lazy)[1], coef(fit_lazy)[2],
                        coef(fit_nonlazy)[1], coef(fit_nonlazy)[2],
                        coef(fit_lazy_nf)[1], coef(fit_lazy_nf)[2]), 
       x="trial_length",  y=sprintf("time of 10K reps (ms)")) + 
  expand_limits(x=0, y=0) #+ scale_x_continuous(breaks=0:500)

ggplot(read_ds('../lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", nwlcalls < 100),
       aes(as.factor(nwlcalls), time_ms, fill=item)) + 
  geom_bar(stat='identity', position='dodge') + 
  facet_wrap(~s) +
  labs(title = "absolute times for s=100M and s=1G") + 
  xlab("trial_length") + ylab("time (ms)") +
  theme(legend.position=c(.15, .8))

rm(sandbox_ds, fit_lazy, fit_nonlazy, fit_lazy_nf)
```

## Check
In the experiments above we ran the program with the "lazy" or "non-lazy" flag and measured. The total time of each experiment can be written as $t_l = l_l + a$ and  $t_n = l_n + a$ for the two versions respectively; only the $t$s being known. Furthermore, we have  $\hat{l}_l$ and $\hat{l}_n$ estimations -- computed by combining the time / wl call with the number of with the count of wl calls in each input (Section "Input Properties"). Hence we should expect 

$$
\delta t = t_l - t_n = l_l + a - l_n - a = l_l - l_n \approx \delta\hat{l} = \hat{l}_l - \hat{l}_n
$$


```{r, echo=FALSE, warning=FALSE, message=FALSE}
read_lazy_call_cnt <- function(path = '../lazy_vs_nonlazy_data/lazy.csv'){
  lazy_calls <- sapply(0:3000, function(i) sprintf("consecutive_lazy_wl_calls%d", i))
  ds <- (read_ds(path) %>%
           filter(item %in% lazy_calls, label=="lazy") %>%
           mutate(count = value) %>%
           group_by(b_path, item) %>%
           summarise(len_s = mean(len_s),
                     len_t = mean(len_t),
                     value = mean(value),
                     count = mean(count)))
  ds$nwlcalls = as.numeric(sapply(ds$item, function(s) substr(s, 26, 300)))
  ds
}

sandbox_timing_path <- '../lazy_vs_nonlazy_data/sandbox_timing.csv'
fit_lazy_g <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                   filter(item != "dstruct", s==1e9, item=="lazy"))
fit_nonlazy_g <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                      filter(item != "dstruct", s==1e9, item!="lazy"))
fit_lazy_m <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                   filter(item != "dstruct", s==1e8, item=="lazy"))
fit_nonlazy_m <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                      filter(item != "dstruct", s==1e8, item!="lazy"))

wl_time_ds <- rbind(filter(read_lazy_call_cnt(), len_s == 1e9) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000), 
                    filter(read_lazy_call_cnt(), len_s == 1e8) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000))


kable(merge(wl_time_ds %>% 
        mutate(delta_l_hat = round(lazy - nonlazy, 2), 
               l_l=round(lazy, 2), l_n=round(nonlazy/1, 2)) %>% 
        select(b_path, delta_l_hat, l_l, l_n),
      spread(read_ds('../lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
              filter(measuring == "time", item == "ms_bvector") %>% 
              select(b_path, label, value) %>%
              group_by(b_path, label) %>% 
              summarise(value=mean(value)), key = label, value = value) %>%
        mutate(delta_t = round((lazy - nonlazy) / 1000, 2),
               t_l = round(lazy/1000, 2), t_n = round(nonlazy/1000, 2)) %>% 
        select(b_path, t_l, t_n, delta_t)) %>% select(b_path, t_l, t_n, l_l, l_n, delta_t, delta_l_hat), digits=2, row.names=FALSE)




ccc <-  round(with(merge(wl_time_ds %>% 
                           mutate(pred_abs_diff_sec = round(lazy - nonlazy, 2)) %>% 
                           select(b_path, pred_abs_diff_sec),
                         spread(read_ds('../lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
                                  filter(measuring == "time", item == "ms_bvector") %>% 
                                  select(b_path, label, value) %>%
                                  group_by(b_path, label) %>% 
                                  summarise(value=mean(value)), 
                                key = label, value = value) %>%
                           mutate(actual_abs_diff_sec = round((lazy - nonlazy) / 1000, 2)) %>% 
                           select(b_path, actual_abs_diff_sec)), 
                   cor(pred_abs_diff_sec, actual_abs_diff_sec)), 2)
rm(wl_time_ds, fit_nonlazy_m, fit_lazy_m, fit_nonlazy_g, fit_lazy_g, sandbox_timing_path)
```


The numbers are not identical (process dependent factors might influence the running time of function calls), but they are correlated ($corr(\delta t, \delta\hat{l})$ = `r ccc`).
```{r, echo=FALSE, warning=FALSE, message=FALSE}
rm(ccc)
```

\newpage
# Double rank and fail
## Code
```
// Given subtree_double_rank(v, i, j) -> (a.first, a.second) -- to simplify code

// DOUBLE RANK: int i, int j, char c
p = bit_path(c)
result_i, result_j = i, j;
node_type v = m_tree.root();
for (l = 0; l < path_len; ++l, p >>= 1) {
  a = subtree_double_rank(v, m_tree.bv_pos(v) + result_i, m_tree.bv_pos(v) + result_j);

  if(p&1){ // left child
      if(result_i > 0) result_i = a.first; 
      if(result_j > 0) result_j = a.second; 
  } else { // right child
      if(result_i > 0) result_i -= a.first;
      if(result_j > 0) result_j -= a.second;
  }   
  v = m_tree.child(v, p&1); // goto child
}   
return(result_i, result_j)

// DOUBLE RANK AND FAIL
p = bit_path(c)
result_i, result_j = i, j;
node_type v = m_tree.root();
for (l = 0; l < path_len; ++l, p >>= 1) {
  a = subtree_double_rank(v, m_tree.bv_pos(v) + result_i, m_tree.bv_pos(v) + result_j);

  if(p&1){ // left child
      if(result_i > 0) result_i = a.first; 
      if(result_j > 0) result_j = a.second; 
  } else { // right child
      if(result_i > 0) result_i -= a.first;
      if(result_j > 0) result_j -= a.second;
  }   
  if(result_i == result_j) // Weiner Link call will fail
    return(0, 0)
  v = m_tree.child(v, p&1); // goto child
}
return(result_i, result_j)
```
\newpage
## Performance
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
#ddd <- rbind(read_ds('../double_rank_and_fail/double_rank_and_fail.csv') %>% mutate(label="fail"),
#      read_ds('../double_rank_and_fail/double_rank.csv') %>% mutate(label="no_fail")) %>% #mutate(time_ms = value) %>% filter(measuring == "time", item %in% c("ms_bvector", "runs_bvector"))

ddd <- read_ds('../double_rank_and_fail/res.csv') %>% mutate(time_ms = value) %>% filter(measuring == "time", item %in% c("ms_bvector", "runs_bvector"))

dd <- group_by(ddd, item, label, b_path) %>% summarise(avg_time = mean(time_ms), sd_time = sd(time_ms))
kable(dd, digits=2, caption="Time (in ms) of 500K calls to `wl()` based on `single_rank()` or `double_rank()` methods on 100MB random DNA input; Mean/sd over 20 repetitions.")


rank_comp <- spread(dd %>% select(item, label, avg_time), key = label, value = avg_time) %>% mutate(abs_ratio = double_rank_and_fail / double_rank, rel_ratio = 100 * abs(double_rank_and_fail - double_rank) / double_rank)
kable(data.frame(rank_comp),
      digits=2, caption="Single vs. double rank. Absolute (double / single) and relative (100 * |double - single| / single) ratios of average times.", row.names=FALSE)

ggplot(ddd, aes(item, time_ms, color=label)) + geom_boxplot() + geom_jitter(width=0.1, alpha=0.3) + ylab('total time (ms)') + facet_wrap(~b_path)
rm(dd, ddd, rank_comp)
```


\newpage
# Parallelization
## Code 
See the pseudo-code in the repo ([link](https://github.com/odenas/indexed_ms/blob/master/notes/reupdate/parallel_MS.pdf))

## Performance
Run the MS construction program on the same input (random strings $s$ of length 100M and $t$ of length 5M) with varying parallelzation degree (nthreads = number of threads).

The time is reported over 5 runs for each fixed number of threads.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5}
read_all <- function() do.call(rbind, lapply(list.files('../parallelization_data/', 'results.*.threads.csv'), function(n) read_ds(sprintf('../parallelization_data/%s', n), TRUE)))

aa <- read_all() %>% filter(measuring == "time")
#aa$b_path <- simplify2array(lapply(strsplit(aa$b_path, "_", fixed = TRUE), parse_b_path))

ggplot(aa, aes(factor(nthreads), value/1000, color=inp_type)) + geom_boxplot() + facet_wrap(~item, scales = "free_y", nrow = 3) + ylab('seconds') + labs(title = "Time usage")
rm(aa)
```

Space in MB for the same settings as above. 

Each thread allocates its own $ms$ vector with initial size $|t| / nthreads$ then it resizes by a factor of 1.5 each time it needs to. Resizing will always result in a vector smaller than $2|t|$ elements.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
alloc_str <- sapply(0:30, function(i) sprintf("ms_bvector_allocated%d", i))
used_str <- sapply(0:30, function(i) sprintf("ms_bvector_used%d", i))

aa <- (read_all() %>% 
         filter(measuring == "space", item %in% alloc_str) %>% 
         group_by(len_s, len_t, measuring, label, b_path, nthreads) %>% 
         summarise(value = mean(value), item="ms_bvector_allocated") %>% 
         mutate(value = value * nthreads))

bb <- (read_all() %>% 
         filter(measuring == "space", item %in% used_str) %>%
         group_by(len_s, len_t, measuring, label, b_path, nthreads, item) %>% 
         summarise(value = mean(value)) %>% 
         group_by(len_s, len_t, measuring, label, b_path, nthreads) %>% 
         summarise(value = sum(value), item="ms_bvector_used"))


ggplot(rbind(aa, bb), aes(factor(nthreads), value / 1000000, fill=item, group=item, color=item)) + geom_bar(stat='identity', position='dodge')  + ylab('MB') + labs(title = "Space usage")
rm(aa, alloc_str, bb, read_all, used_str)
```

