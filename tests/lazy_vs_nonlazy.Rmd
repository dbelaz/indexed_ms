---
title: "Lazy vs. non lazy"
author: "O. Denas"
date: "12/28/2016"
output:
  pdf_document: 
    number_sections: yes
    toc: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Double vs. single rank
```{r, echo=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
source('utils.R')
rds <- (read_ds('lazy_vs_nonlazy_data/rank_timing.csv') %>% select(nwlcalls, item, time_ms) %>% filter(item != "dstruct"))


dd <- read_ds('lazy_vs_nonlazy_data/rank_timing.csv') %>% filter(item != "dstruct") %>% group_by(item) %>% summarise(avg_time = mean(time_ms), sd_time = sd(time_ms))
kable(dd, digits=2, caption="Time (in ms) of 500K calls to `wl()` based on `single_rank()` or `double_rank()` methods on 100MB random DNA input; Mean/sd over 20 repetitions.")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
kable(dcast(dd, .~item, value.var="avg_time") %>% 
        mutate(abs_ratio = double_rank / single_rank,
               rel_ratio = 100 * abs(double_rank - single_rank) / single_rank),
      digits=2, caption="Absolute (double_rank / single_rank) and relative (100 * |double_rank - single_rank| / single_rank) ratios of average times from the above table.", row.names=FALSE)

ggplot(read_ds('lazy_vs_nonlazy_data/rank_timing.csv') %>% filter(item != "dstruct"), aes(item, time_ms)) + geom_boxplot() + geom_jitter(alpha=.3, width = 0.2) + ylab('total time (ms)')
```

# Lazy vs non-lazy
## Input properties

For various types ("mut_XMs_YMt_Z" means `s` and `t` are random identical strings of length X, and Y million respectively with mutations inserted every Z characters. "rnd_XMs_YMt" means `s` and `t` are random strings of length X, and Y million respectively) of inputs run the MS algorithm and count the number of consecutive `lazy_wl()` calls.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(read_lazy_call_cnt(), aes(factor(nwlcalls), count)) + 
  geom_bar(stat='identity') + 
  facet_wrap(~b_path, scale='free') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5))
```

\newpage
## Code
The lazy and non-lazy versions differ in a couple of lines of code as follows

```
if(flags.lazy){
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.lazy_wl(v, c); 
            h_star++;
        }   
    }   
    if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
        st.lazy_wl_followup(v);
} else { // non-lazy weiner links
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.lazy_wl(v, c); 
            h_star++;
        }   
    }   
}
```
\newpage
## Run time
```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(read_time_ds(), aes(b_path, value/1000, color=label)) + 
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(shape=1, alpha=0.5, width=0.1) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position=c(.9, .8)) + 
  facet_wrap(~item) +
  labs(title = "MS vector construction time") + ylab('seconds') + xlab("input type") 
```

The right panel shows the time to construct the `runs` vector. This stage is the same for both versions and is shown as a control. On the left panel it can be seen that speedup correlates positively with both the size of the indexed string and the mutation period.

\newpage
## Sandbox timing
Measure the time of 10k repetitions of 

 * (lazy) $n$ consecutive `lazy_wl()` calls followed by a `lazy_wl_followup()`
 * (nonlazy) $n$ consecutive `wl()` calls
 * (lazy_nf) $n$ consecutive `lazy_wl()` calls

```
// lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
    st.lazy_wl_followup(v);
...
// non-lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.wl(v, s_rev[k--]);
...
// lazy_nf
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
sandbox_ds <- read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", s==1e9, nwlcalls<100)

fit_lazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy")))
fit_nonlazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="nonlazy")))
fit_lazy_nf <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy_nf")))

ggplot(sandbox_ds, aes(nwlcalls, time_ms,color=item)) + geom_point() + 
  geom_abline(intercept = coef(fit_lazy)[1], slope = coef(fit_lazy)[2], color='red', alpha=0.5) + 
  geom_abline(intercept = coef(fit_nonlazy)[1], slope = coef(fit_nonlazy)[2], color='blue', alpha=0.5) +
  geom_abline(intercept = coef(fit_lazy_nf)[1], slope = coef(fit_lazy_nf)[2], color='green', alpha=0.5) +
  labs(title = "indexed input size 1G", 
       subtitle=sprintf("lazy: %.2f + %.4f*n; nonlazy: %.2f + %.4f*n; lazy_nf: %.2f + %.4f*n", 
                        coef(fit_lazy)[1], coef(fit_lazy)[2],
                        coef(fit_nonlazy)[1], coef(fit_nonlazy)[2],
                        coef(fit_lazy_nf)[1], coef(fit_lazy_nf)[2]), 
       x="trial_length",  y=sprintf("time of 10K reps (ms)")) + 
  expand_limits(x=0, y=0) #+ scale_x_continuous(breaks=0:500)

ggplot(read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", nwlcalls < 100),
       aes(as.factor(nwlcalls), time_ms, fill=item)) + 
  geom_bar(stat='identity', position='dodge') + 
  facet_wrap(~s) +
  labs(title = "absolute times for s=100M and s=1G") + 
  xlab("trial_length") + ylab("time (ms)") +
  theme(legend.position=c(.15, .8))

rm(fit_lazy, fit_nonlazy, fit_lazy_nf)
```

## Check
In the experiments above we ran the program with the "lazy" or "non-lazy" flag and measured. The total time of each experiment can be written as $t_l = l_l + a$ and  $t_n = l_n + a$ for the two versions respectively; only the $t$s being known. Furthermore, we have  $\hat{l}_l$ and $\hat{l}_n$ estimations -- computed by combining the time / wl call with the number of with the count of wl calls in each input (Section "Input Properties"). Hence we should expect 

$$
\delta t = t_l - t_n = l_l + a - l_n - a = l_l - l_n \approx \delta\hat{l} = \hat{l}_l - \hat{l}_n
$$


```{r, echo=FALSE, warning=FALSE, message=FALSE}
fit_lazy_g <- lm(time_ms ~ nwlcalls,
                 read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                   filter(item != "dstruct", s==1e9, item=="lazy"))
fit_nonlazy_g <- lm(time_ms ~ nwlcalls,
                    read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                      filter(item != "dstruct", s==1e9, item!="lazy"))
fit_lazy_m <- lm(time_ms ~ nwlcalls,
                 read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                   filter(item != "dstruct", s==1e8, item=="lazy"))
fit_nonlazy_m <- lm(time_ms ~ nwlcalls,
                    read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                      filter(item != "dstruct", s==1e8, item!="lazy"))

wl_time_ds <- rbind(filter(read_lazy_call_cnt(), len_s == 1e9) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000), 
                    filter(read_lazy_call_cnt(), len_s == 1e8) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000))

kable(merge(wl_time_ds %>% 
        mutate(delta_l_hat = round(lazy - nonlazy, 2), 
               l_l=round(lazy, 2), l_n=round(nonlazy/1, 2)) %>% 
        select(b_path, delta_l_hat, l_l, l_n),
      dcast(read_ds('lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
              filter(measuring == "time", item == "ms_alg") %>% 
              select(b_path, label, value) %>%
              group_by(b_path, label) %>% 
              summarise(value=mean(value)), b_path~label) %>%
        mutate(delta_t = round((lazy - nonlazy) / 1000, 2),
               t_l = round(lazy/1000, 2), t_n = round(nonlazy/1000, 2)) %>% 
        select(b_path, t_l, t_n, delta_t)) %>% select(b_path, t_l, t_n, l_l, l_n, delta_t, delta_l_hat), digits=2, row.names=FALSE)

ccc <-  round(with(merge(wl_time_ds %>% 
        mutate(pred_abs_diff_sec = round(lazy - nonlazy, 2)) %>% 
        select(b_path, pred_abs_diff_sec),
      dcast(read_ds('lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
              filter(measuring == "time", item == "ms_alg") %>% 
              select(b_path, label, value) %>%
              group_by(b_path, label) %>% 
              summarise(value=mean(value)), b_path~label) %>%
        mutate(actual_abs_diff_sec = round((lazy - nonlazy) / 1000, 2)) %>% 
        select(b_path, actual_abs_diff_sec)), cor(pred_abs_diff_sec, actual_abs_diff_sec)), 2)
```

The numbers are not identical (process dependent factors might influence the running time of function calls), but they are correlated ($corr(\delta t, \delta\hat{l})$ = `r ccc`).


