---
title: "Report of experiments"
author: "O. Denas"
date: "12/28/2016"
output:
  pdf_document: 
    number_sections: yes
    toc: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\newpage
# Input properties

For various types ("mut_XMs_YMt_Z" means `s` and `t` are random identical strings of length X, and Y million respectively with mutations inserted every Z characters. "rnd_XMs_YMt" means `s` and `t` are random strings of length X, and Y million respectively) of inputs run the MS algorithm and count the number of consecutive `wl()` or `parent()` calls during the $\mathtt{runs}$ or $\mathtt{ms}$ construction phase.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
source('utils.R')

ggplot(read_stats_ds() %>% filter(call_name=="wl"), aes(factor(ncalls), value)) + geom_bar(stat='identity') + facet_wrap(vec~b_path, scale='free') + theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5),
        strip.text.x = element_text(size = 8)) + labs(title = "Counting wl() calls") 

ggplot(read_stats_ds() %>% filter(call_name=="parent"), aes(factor(ncalls), value)) + geom_bar(stat='identity') + facet_wrap(~b_path, scale='free') + theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5), strip.text.x = element_text(size = 8)) + labs(title = "Counting parent() calls")
```

\newpage

\newpage
# Double vs. single rank
## Code
The single rank and double rank implementations in sdsl: rank_support_v.hpp [link](https://github.com/odenas/indexed_ms/blob/master/sdsl-lite/include/sdsl/rank_support_v.hpp)

```
// RANK(idx)
const uint64_t* p = m_basic_block.data() + ((idx>>8)&0xFFFFFFFFFFFFFFFEULL);
return *p + ((*(p+1)>>(63 - 9*((idx&0x1FF)>>6)))&0x1FF) +
      (idx&0x3F ? trait_type::word_rank(m_v->data(), idx) : 0);

// DOUBLE RANK(i, j)
if((i>>8) == (j>>8)){
  const uint64_t* p = m_basic_block.data() + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
  res.first  = *p + ((*(p+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
            (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0); 
  res.second = *p + ((*(p+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
            (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0); 
} else {
  const uint64_t* p = m_basic_block.data() + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
  res.first  = *p + ((*(p+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
            (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0); 
  p -= (((i>>8)&0xFFFFFFFFFFFFFFFEULL) - ((j>>8)&0xFFFFFFFFFFFFFFFEULL));
  res.second = *p + ((*(p+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
            (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0); 
}
return res;
```

\newpage
## Performance

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#rds <- (read_ds('lazy_vs_nonlazy_data/rank_timing.csv') %>% select(nwlcalls, item, time_ms) %>% filter(item != "dstruct"))


dd <- read_ds('lazy_vs_nonlazy_data/rank_timing.csv') %>% filter(item != "dstruct") %>% group_by(item) %>% summarise(avg_time = mean(time_ms), sd_time = sd(time_ms))
kable(dd, digits=2, caption="Time (in ms) of 500K calls to `wl()` based on `single_rank()` or `double_rank()` methods on 100MB random DNA input; Mean/sd over 20 repetitions.")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
kable(rbind(data.frame(double_rank=546.89, single_rank=572.74, abs_ratio=0.95, rel_ratio=4.51, when="before"), 
            data.frame(dcast(dd, .~item, value.var="avg_time") %>% 
        mutate(abs_ratio = double_rank / single_rank,
               rel_ratio = 100 * abs(double_rank - single_rank) / single_rank,
               when="after"))[,2:6]),
      digits=2, caption="Absolute (double_rank / single_rank) and relative (100 * |double_rank - single_rank| / single_rank) ratios of average times from the above table.", row.names=FALSE)

#kable(dcast(dd, .~item, value.var="avg_time") %>% 
#        mutate(abs_ratio = double_rank / single_rank,
#               rel_ratio = 100 * abs(double_rank - single_rank) / single_rank),
#      digits=2, caption="Absolute (double_rank / single_rank) and relative (100 * |double_rank - single_rank| / single_rank) ratios of average times from the above table.", row.names=FALSE)

ggplot(read_ds('lazy_vs_nonlazy_data/rank_timing.csv') %>% filter(item != "dstruct"), aes(item, time_ms)) + geom_boxplot() + geom_jitter(alpha=.3, width = 0.2) + ylab('total time (ms)')
```
\newpage

# Lazy vs non-lazy
## Code
The lazy and non-lazy versions differ in a couple of lines of code as follows

```
if(flags.lazy){
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.lazy_wl(v, c); 
            h_star++;
        }   
    }   
    if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
        st.lazy_wl_followup(v);
} else { // non-lazy weiner links
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.wl(v, c); 
            h_star++;
        }   
    }   
}
```
\newpage
## Run time
```{r, echo=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
source('utils.R')

dds <- read_time_ds() %>% filter(item!="runs_bvector")
dds$b_path <- factor(dds$b_path, levels = (sorted_bpaths(dds$b_path) %>% 
                                             arrange(inp_type, mu_nr))$b_path)

ggplot(dds, aes(b_path, value/1000, color=label)) + 
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(shape=1, alpha=0.5, width=0.1) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position=c(.9, .8)) +
  #facet_wrap(~item) +
  labs(title = "MS vector construction time") + ylab('seconds') + xlab("input type") 
rm(dds)
```

The right panel shows the time to construct the `runs` vector. This stage is the same for both versions and is shown as a control. On the left panel it can be seen that speedup correlates positively with both the size of the indexed string and the mutation period.

\newpage
## Sandbox timing
Measure the time of 10k repetitions of 

 * (lazy) $n$ consecutive `lazy_wl()` calls followed by a `lazy_wl_followup()`
 * (nonlazy) $n$ consecutive `wl()` calls
 * (lazy_nf) $n$ consecutive `lazy_wl()` calls

```
// lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
    st.lazy_wl_followup(v);
...
// non-lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.wl(v, s_rev[k--]);
...
// lazy_nf
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
sandbox_ds <- read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", s==1e9, nwlcalls<100)

fit_lazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy")))
fit_nonlazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="nonlazy")))
fit_lazy_nf <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy_nf")))

ggplot(sandbox_ds, aes(nwlcalls, time_ms,color=item)) + geom_point() + 
  geom_abline(intercept = coef(fit_lazy)[1], slope = coef(fit_lazy)[2], color='red', alpha=0.5) + 
  geom_abline(intercept = coef(fit_nonlazy)[1], slope = coef(fit_nonlazy)[2], color='blue', alpha=0.5) +
  geom_abline(intercept = coef(fit_lazy_nf)[1], slope = coef(fit_lazy_nf)[2], color='green', alpha=0.5) +
  labs(title = "indexed input size 1G", 
       subtitle=sprintf("lazy: %.2f + %.4f*n; nonlazy: %.2f + %.4f*n; lazy_nf: %.2f + %.4f*n", 
                        coef(fit_lazy)[1], coef(fit_lazy)[2],
                        coef(fit_nonlazy)[1], coef(fit_nonlazy)[2],
                        coef(fit_lazy_nf)[1], coef(fit_lazy_nf)[2]), 
       x="trial_length",  y=sprintf("time of 10K reps (ms)")) + 
  expand_limits(x=0, y=0) #+ scale_x_continuous(breaks=0:500)

ggplot(read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", nwlcalls < 100),
       aes(as.factor(nwlcalls), time_ms, fill=item)) + 
  geom_bar(stat='identity', position='dodge') + 
  facet_wrap(~s) +
  labs(title = "absolute times for s=100M and s=1G") + 
  xlab("trial_length") + ylab("time (ms)") +
  theme(legend.position=c(.15, .8))

rm(fit_lazy, fit_nonlazy, fit_lazy_nf)
```

## Check
In the experiments above we ran the program with the "lazy" or "non-lazy" flag and measured. The total time of each experiment can be written as $t_l = l_l + a$ and  $t_n = l_n + a$ for the two versions respectively; only the $t$s being known. Furthermore, we have  $\hat{l}_l$ and $\hat{l}_n$ estimations -- computed by combining the time / wl call with the number of with the count of wl calls in each input (Section "Input Properties"). Hence we should expect 

$$
\delta t = t_l - t_n = l_l + a - l_n - a = l_l - l_n \approx \delta\hat{l} = \hat{l}_l - \hat{l}_n
$$


```{r, echo=FALSE, warning=FALSE, message=FALSE}
fit_lazy_g <- lm(time_ms ~ nwlcalls,
                 read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                   filter(item != "dstruct", s==1e9, item=="lazy"))
fit_nonlazy_g <- lm(time_ms ~ nwlcalls,
                    read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                      filter(item != "dstruct", s==1e9, item!="lazy"))
fit_lazy_m <- lm(time_ms ~ nwlcalls,
                 read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                   filter(item != "dstruct", s==1e8, item=="lazy"))
fit_nonlazy_m <- lm(time_ms ~ nwlcalls,
                    read_ds('lazy_vs_nonlazy_data/sandbox_timing.csv') %>% 
                      filter(item != "dstruct", s==1e8, item!="lazy"))

wl_time_ds <- rbind(filter(read_lazy_call_cnt(), len_s == 1e9) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000), 
                    filter(read_lazy_call_cnt(), len_s == 1e8) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000))

kable(merge(wl_time_ds %>% 
        mutate(delta_l_hat = round(lazy - nonlazy, 2), 
               l_l=round(lazy, 2), l_n=round(nonlazy/1, 2)) %>% 
        select(b_path, delta_l_hat, l_l, l_n),
      dcast(read_ds('lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
              filter(measuring == "time", item == "ms_bvector") %>% 
              select(b_path, label, value) %>%
              group_by(b_path, label) %>% 
              summarise(value=mean(value)), b_path~label) %>%
        mutate(delta_t = round((lazy - nonlazy) / 1000, 2),
               t_l = round(lazy/1000, 2), t_n = round(nonlazy/1000, 2)) %>% 
        select(b_path, t_l, t_n, delta_t)) %>% select(b_path, t_l, t_n, l_l, l_n, delta_t, delta_l_hat), digits=2, row.names=FALSE)

ccc <-  round(with(merge(wl_time_ds %>% 
        mutate(pred_abs_diff_sec = round(lazy - nonlazy, 2)) %>% 
        select(b_path, pred_abs_diff_sec),
      dcast(read_ds('lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
              filter(measuring == "time", item == "ms_bvector") %>% 
              select(b_path, label, value) %>%
              group_by(b_path, label) %>% 
              summarise(value=mean(value)), b_path~label) %>%
        mutate(actual_abs_diff_sec = round((lazy - nonlazy) / 1000, 2)) %>% 
        select(b_path, actual_abs_diff_sec)), cor(pred_abs_diff_sec, actual_abs_diff_sec)), 2)
```

The numbers are not identical (process dependent factors might influence the running time of function calls), but they are correlated ($corr(\delta t, \delta\hat{l})$ = `r ccc`).

\newpage
# Parallelization
Run the MS construction program on the same input (random strings $s$ of length 100M and $t$ of length 5M) with varying parallelzation degree (nthreads = number of threads).

The time is reported over 5 runs for each fixed number of threads.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
rm(list=ls())
source('utils.R')

lazy_calls <- sapply(0:3000, function(i) sprintf("consecutive_lazy_wl_calls%d", i))
read_all <- function() do.call(rbind, lapply(list.files('parallelization_data/', 'results.*.threads.csv'), function(n) read_ds(sprintf('parallelization_data/%s', n))))

ggplot(read_all() %>% filter(measuring == "time", !(item %in% lazy_calls)), aes(factor(nthreads), value/1000)) + geom_boxplot() + facet_wrap(~item, scales = "free_y") + ylab('seconds') + labs(title = "Time usage")
```

Space in MB for the same settings as above. 

Each thread allocates its own $ms$ vector with initial size $|t| / nthreads$ then it resizes by a factor of 1.5 each time it needs to. Resizing will always result in a vector smaller than $2|t|$ elements.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
alloc_str <- sapply(0:30, function(i) sprintf("ms_bvector_allocated%d", i))
used_str <- sapply(0:30, function(i) sprintf("ms_bvector_used%d", i))


aa <- (read_all() %>% 
         filter(measuring == "space", item %in% alloc_str) %>% 
         group_by(len_s, len_t, measuring, label, b_path, nthreads) %>% 
         summarise(value = mean(value), item="ms_bvector_allocated") %>% 
         mutate(value = value * nthreads))

bb <- (read_all() %>% 
         filter(measuring == "space", item %in% used_str) %>%
         group_by(len_s, len_t, measuring, label, b_path, nthreads, item) %>% 
         summarise(value = mean(value)) %>% 
         group_by(len_s, len_t, measuring, label, b_path, nthreads) %>% 
         summarise(value = sum(value), item="ms_bvector_used"))

ggplot(rbind(aa, bb), aes(factor(nthreads), value / 1000000, fill=item, group=item, color=item)) + geom_bar(stat='identity', position='dodge')  + ylab('MB') + labs(title = "Space usage")
```

